from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, classification_report,
                              precision_score, recall_score, f1_score, 
                              confusion_matrix, roc_curve, auc)
from sklearn.preprocessing import label_binarize
from aeon.registry import all_estimators
from aeon.classification.compose import WeightedEnsembleClassifier
from sklearn.preprocessing import LabelEncoder as le

from json import load
import importlib
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import datetime
import os
import re


class AllEstimators:
        
    def __init__(self, 
                 config: dict,
                 X: np.ndarray,
                 Y: np.ndarray,
                 name: str
                 ):
        self.label_code = le()
        self.X = X
        self.Y = Y
        self.config = config
        self.estimators = self.intilize_estimators()
        self.weights = self.intilize_weights(self.estimators)
        self.name = name
        
        
    
    @staticmethod
    def intilize_estimators():
        """
        Initializes the estimators.
        
        Returns:
            self: The updated instance of the `TimeSeriesClassifierForest` class.
        """
        config = load(open("C:\\Users\\Vlad Titov\\Desktop\\Work\\fault_location_machine_learning\\config\\config.json"))["Estimator Name"]

        estimators_list = all_estimators(
                filter_tags={"capability:multivariate": True},
                estimator_types="classifier",
                as_dataframe=True,
                )

        tmp = []
        for idx in range (len(estimators_list)):
            try:
                base_name = str(estimators_list.estimator[idx]).split(".")[2]
                module_name = str(estimators_list.estimator[idx]).split(".")[3]
                estimator_name = estimators_list.name[idx]
                if (
                    estimator_name in config and estimator_name != "DummyClassifier"
                ):
                    estimator_module = importlib.import_module(f"aeon.classification.{base_name}.{module_name}")
                    estimator_class = getattr(estimator_module, estimator_name)
                    estimator = estimator_class()
                    try:
                        tmp.append([estimator_name, estimator])
                    except Exception as e:
                        print(f"Error with {estimators_list.name[idx]}")
                        print(e)

                elif estimator_name == "DummyClassifier" and estimator_name in config:
                    estimator_module = importlib.import_module(f"aeon.classification.{base_name}")
                    estimator_class = getattr(estimator_module, estimator_name)
                    estimator = estimator_class()
                    tmp.append([estimator_name, estimator])
            except Exception as e:
                print(f"Error with {estimators_list.name[idx]}")
                print(e)
                continue
        return np.array(tmp)
    
    @staticmethod
    def intilize_weights(est_list):
        """
        Initializes the weights for a list of estimators.
        
        Args:
            est_list (list): A list of estimators.
        
        Returns:
            numpy.ndarray: An array of weights, where each weight is 1.
        """
                
        weights = [1] * len(est_list)
        return np.array(weights)
    
    def encoder_preprocessing_fit(self, input_data):
        """
        Preprocesses the input data by splitting it into training and testing sets.
        
        Args:
            input_data (numpy.ndarray): The input data.
            
        Returns:
            numpy.ndarray: The preprocessed input data.
        """
        fitted_enc = self.label_code.fit(input_data)
        out_data = fitted_enc.transform(input_data)
        return out_data, fitted_enc
    
    def encoder_preprocessing_trans(self, input_data, fitted_enc):
        """
        Preprocesses the input data by splitting it into training and testing sets.
        
        Args:
            input_data (numpy.ndarray): The input data.
            
        Returns:
            numpy.ndarray: The preprocessed input data.
        """
        out_data = fitted_enc.transform(input_data)
        return out_data
    
    def decoder_preprocessing(self, input_data, fitted_enc):
        """
        Preprocesses the input data by splitting it into training and testing sets.
        
        Args:
            input_data (numpy.ndarray): The input data.
            
        Returns:
            numpy.ndarray: The preprocessed input data.
        """
        out_data = fitted_enc.inverse_transform(input_data)
        return out_data

    def data_preprocess(self):
        """
        Preprocesses the input data by splitting it into training and testing sets.
        
        Args:
            X (numpy.ndarray): The input data.
            Y (numpy.ndarray): The target labels.
        
        Returns:
            self: The updated instance of the `WeightedEnsembleClassifier` class.
        """
        X = self.X
        Y = self.Y

        marker = np.array(range(len(Y)))

        print(f"Uniq values in Y: {np.unique(Y)}")

        X_train, X_test, Y_train, Y_test, _, marker_test = train_test_split(X, 
                                                            Y,
                                                            marker,
                                                            test_size=0.2
                                                            )
        self.X_train = X_train
        self.X_test = X_test

        print(f"Unique target labels pre: {np.unique(Y_train)}")
        print(f"Unique target test labels pre: {np.unique(Y_test)}")

        self.Y_train, self.fitted_enc = self.encoder_preprocessing_fit(Y_train)
        self.Y_test = self.encoder_preprocessing_trans(Y_test, self.fitted_enc)
        self.marker_test = marker_test

        print(f"Unique target labels: {np.unique(self.Y_train)}")
        print(f"Unique target test labels: {np.unique(self.Y_test)}")
        
        return self
    
    def train_model(self):
        """
        Trains the time series classification model using the provided configuration.
        
        Args:
            X_train (numpy.ndarray): The training input data.
            Y_train (numpy.ndarray): The training target labels.
            config (dict): The configuration parameters for the model.
        
        Returns:
            self: The updated instance of the `WeightedEnsembleClassifier` class.
        """
                
        X_train = self.X_train
        Y_train = self.Y_train
        #config = self.config

        model = WeightedEnsembleClassifier(
            classifiers = list(self.estimators[:,1]),
            weights=self.weights)
        model.fit(X_train, Y_train)
        self.model = model
        return self

    def test_model(self):
        """
        Evaluates the performance of the trained time series classification model on the test dataset.
        
        Args:
            self (TimeSeriesClassifierForest): The instance of the `TimeSeriesClassifierForest` class.
        
        Returns:
            self (TimeSeriesClassifierForest): The updated instance of the `TimeSeriesClassifierForest` class.
        """
                
        X_test = self.X_test
        Y_test = self.Y_test
        model = self.model

        Y_pred = model.predict(X_test)

        print(
            "Unique classes in Y_test:", np.unique(Y_test), 
            "\n",
            "Unique classes in Y_pred:",np.unique(Y_pred),
        )

        Y_pred_dec = self.decoder_preprocessing(Y_pred, self.fitted_enc)
        Y_test_dec = self.decoder_preprocessing(Y_test, self.fitted_enc)
        print("Decoded Y_test:", Y_test_dec, "\n", "Decoded Y_pred:",Y_pred_dec)

        Y_pred_proba = model.predict_proba(X_test)

        Y_init_name = self.model.classes_

        print(f"Initiall class names: {self.decoder_preprocessing(Y_init_name, self.fitted_enc)}")

        accuracy = accuracy_score(Y_test_dec, Y_pred_dec)
        report = classification_report(Y_test, Y_pred, output_dict=True, zero_division=0 )

        print("Accuracy:", accuracy)
        print("\nClassification Report:\n", report)

        self.visualization(Y_test, Y_pred, Y_pred_proba)
        self.test_model_efficiency(Y_pred, Y_pred_proba, self.marker_test)

        return accuracy, report

    def visualization(self, Y_test, Y_pred, Y_pred_proba):
        """
        Visualizes the true and predicted labels for the test dataset.
        
        This method creates a scatter plot that displays the true labels (marked with green asterisks) and the predicted labels (marked with red circles) for the test dataset. The plot is labeled with the x-axis as "Sample Index" and the y-axis as "Label", and the title is "True vs Predicted Labels". The legend distinguishes between the true and predicted labels.
        """
        # Визуализация результатов
        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(30, 20))
        #fig.suptitle("Time Series Classification Results", fontsize=30)

        # Истинные и предсказанные метки
        axes[0, 0].scatter(Y_test, Y_pred, marker='o', color="black", 
                           facecolors='black', edgecolors='black', s=30)
        #axes[0, 0].scatter(range(len(Y_pred)), Y_pred, marker='o', facecolors='none', edgecolors='r',
        #                   label="Predicted Labels", color="red", s=30)
        axes[0, 0].plot(Y_test, Y_test, color="red", linewidth=3)
        axes[0, 0].set_xlabel("Target classes", fontsize=30)
        axes[0, 0].set_ylabel("Predict classes", fontsize=30)
        axes[0, 0].set_yticks(np.unique(Y_pred))
        axes[0, 0].set_xticks(np.unique(Y_test))
        axes[0, 0].set_yticklabels(np.unique(Y_pred), fontsize=30)
        axes[0, 0].set_xticklabels(np.unique(Y_test), fontsize=30)
        axes[0, 0].set_xlim(min(np.unique(Y_test)) - 0.5, max(np.unique(Y_test)) + 0.5)
        axes[0, 0].set_ylim(min(np.unique(Y_test)) - 0.5, max(np.unique(Y_test)) + 0.5)
        axes[0, 0].set_title("True vs Predicted Labels", fontsize=30)
        axes[0, 0].legend()

        # Метрики производительности
        precision = precision_score(Y_test, Y_pred, average=None)
        recall = recall_score(Y_test, Y_pred, average=None)
        f1 = f1_score(Y_test, Y_pred, average=None)
        metrics_df = pd.DataFrame({"Precision": precision, "Recall": recall, "F1-Score": f1}, index=np.unique(Y_pred))
        sns.heatmap(metrics_df, annot=True, cmap="YlGnBu", ax=axes[0, 1],
                    annot_kws={"size": 20})
        axes[0, 1].set_title("Performance Metrics", fontsize=25)
        axes[0, 1].set_xlabel("Metrics", fontsize=25)
        axes[0, 1].set_ylabel("Classes", fontsize=25)
        axes[0, 1].tick_params(axis='both', which='major', labelsize=20)
        cbar = axes[0, 1].collections[0].colorbar
        cbar.ax.tick_params(labelsize=20)
        
        # Матрица ошибок
        cm = confusion_matrix(Y_test, Y_pred, labels=self.model.classes_)
        sns.heatmap(cm, annot=True, cmap="YlGnBu", ax=axes[1, 0],
                    annot_kws={"size": 20})
        axes[1, 0].set_title("Confusion Matrix")
        axes[1, 0].set_xlabel("Predicted Label")
        axes[1, 0].set_ylabel("True Label")
        
        # ROC кривая и AUC
        n_classes = len(self.model.classes_)
        Y_test_bin = label_binarize(Y_test, classes=self.model.classes_)

        if n_classes == 2:
            fpr, tpr, _ = roc_curve(Y_test, Y_pred_proba[:, 1])
            roc_auc = auc(fpr, tpr)
            axes[1, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
            axes[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            axes[1, 1].set_xlim([0.0, 1.0])
            axes[1, 1].set_ylim([0.0, 1.05])
            axes[1, 1].set_xlabel('False Positive Rate')
            axes[1, 1].set_ylabel('True Positive Rate')
            axes[1, 1].set_title('Receiver Operating Characteristic (ROC) Curve')
            axes[1, 1].legend(loc="lower right")
        else:
            for i in range(n_classes):
                fpr, tpr, _ = roc_curve(Y_test_bin[:, i], Y_pred_proba[:, i]) # type: ignore
                roc_auc = auc(fpr, tpr)
                axes[1, 1].plot(fpr, tpr, lw=2, label=f'ROC curve of class {i} (AUC = {roc_auc:.2f})')

            axes[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            axes[1, 1].set_xlim([0.0, 1.0])
            axes[1, 1].set_ylim([0.0, 1.05])
            axes[1, 1].set_xlabel('False Positive Rate')
            axes[1, 1].set_ylabel('True Positive Rate')
            axes[1, 1].set_title('Receiver Operating Characteristic (ROC) Curve')
            axes[1, 1].legend(loc="lower right")

        # Сохранение графиков

        adress = "graf\\copmose\\"
        if not os.path.exists(adress):
                os.makedirs(adress)
    
        plt.savefig("graf\\copmose\\classification_results_" 
                    + datetime.datetime.now().strftime("%d_%m_%Y_%H_%M")
                    + ".png", dpi=500, bbox_inches="tight")    

    def test_model_efficiency(self, 
                              Y_pred: np.ndarray, 
                              Y_pred_proba: np.ndarray,
                              marker_test: np.ndarray):
        """
        Evaluates the efficiency of different methods and visualizes the results using a histogram plot.
        
        This function takes in the predicted labels, predicted probabilities, and a marker array for the test set. It then calculates the errors for various methods, including the "one" method, methods for testing, the STO method, and the weighted average method. The results are stored in a dictionary called `meta_result`.
        
        The function also generates separate dictionaries for the weighted average method (`only_predict_result_avg`) and the "one" method (`only_predict_result_one`). These dictionaries are used to generate separate histogram plots for these methods.
        
        Finally, the function calls the `visualization_effiency` method three times, passing in the `meta_result`, `only_predict_result_avg`, and `only_predict_result_one` dictionaries, along with two sets of bin edges (`bins_base` and `bins_hight`). This generates the histogram plots for the different methods and error ranges.
        """
        Y_pred = np.array(
            pd.DataFrame(Y_pred)
            )
        Y_pred_proba = np.array(
            pd.DataFrame(Y_pred_proba)
            )
        marker_test = np.array(
            pd.DataFrame(marker_test)
            )

        fileNameAimReposytory = os.listdir(
            'C:\\Users\\Vlad Titov\\Desktop\\Work\\fault_location_machine_learning\\aim methods'
        )
    
        fileNameAimReposytoryList = re.findall(
            r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z][a-z]|\d|\W|$)|\d+', 
           fileNameAimReposytory[0].split('.')[0])
        
        if 'One' in fileNameAimReposytoryList:

            bins_base = [0, 2.5, np.inf]
            bins_hight = [0, 0.5, 1, 2.5, 5, 10, np.inf]

            columns_name = {
                                "0": "1",
                                "1": "2",
                                "2": "3",
                                "3": "4",
                                "4": "5",
                                "5": "6",
                                "6": "7",
                                "7": "8",
                                "8": "9",
                                "9": "10",
                                "10": "11"
                                }
            
            ErrorsPerMethod = pd.read_csv('Errors per method\\OneSideMethodsErrors.csv')
            ErrorsPerMethod = ErrorsPerMethod.rename(columns=columns_name)
            
            methods_errors_one = self.one_method(
                np.array(ErrorsPerMethod),
                marker_test,
                Y_pred)

            ErrorsPerMethod = np.array(ErrorsPerMethod.drop(['2', '5'], axis=1))
            
            methods_errors = self.methods_for_test(
                ErrorsPerMethod,
                marker_test,
                [1,2,4])

            Result_method_3 = methods_errors[:, 0]
            Result_method_4 = methods_errors[:, 1]
            Result_method_7 = methods_errors[:, 2]

            methods_errors_STO = self.STO_method(
                ErrorsPerMethod, 
                marker_test)

            methods_errors_weight_avg = self.weigth_avg_method(
                ErrorsPerMethod, 
                marker_test, 
                Y_pred_proba)
            
            meta_result = {
                "Result_method_3": Result_method_3,
                "Result_method_4": Result_method_4,
                "Result_method_7": Result_method_7,
                "methods_errors_STO": methods_errors_STO,
                "methods_errors_weight_avg": methods_errors_weight_avg,
                "methods_errors_one": methods_errors_one
            }

            only_predict_result_avg = {
                "methods_errors_weight_avg": methods_errors_weight_avg
            }

            only_predict_result_one = {
                "methods_errors_one": methods_errors_one
                }

        elif "Two" in fileNameAimReposytoryList:

            bins_base = [0, 1, np.inf]
            bins_hight = [0, 0.25, 0.5, 1, 2.5, 5, np.inf]

            columns_name = {
                "0": "1",
                "1": "2",
                "2": "3",
                "3": "4",
                "4": "5",
                "5": "6",
                "6": "7",
                "7": "8",
                "8": "9",
                "9": "10",
                "10": "11",
                "11": "12",
                "12": "13",
                "13": "14",
                "14": "15",
                "15": "16",
                "16": "17",
                "17": "18"
                }

            ErrorsPerMethod = pd.read_csv("Errors per method\\TwoSideMethodsErrors.csv")
            ErrorsPerMethod = ErrorsPerMethod.rename(columns=columns_name)


            methods_errors_one = self.one_method(
                np.array(ErrorsPerMethod), marker_test, np.array(Y_pred)
            )

            methods_errors = self.methods_for_test(
                np.array(ErrorsPerMethod),
                marker_test,
                [6,10,16])

            Result_method_7 = methods_errors[:, 0]
            Result_method_11 = methods_errors[:, 1]
            Result_method_17 = methods_errors[:, 2]
            
            ErrorsPerMethod = np.array(ErrorsPerMethod.drop(["6", "12"], axis=1))
            
            methods_errors_STO = self.STO_method(
                ErrorsPerMethod, 
                marker_test)

            methods_errors_weight_avg = self.weigth_avg_method(
                ErrorsPerMethod, 
                marker_test, 
                Y_pred_proba)
            
            meta_result = {
                "Result_method_7": Result_method_7,
                "Result_method_11": Result_method_11,
                "Result_method_17": Result_method_17,
                "methods_errors_STO": methods_errors_STO,
                "methods_errors_weight_avg": methods_errors_weight_avg,
                "methods_errors_one": methods_errors_one
            }

            only_predict_result_avg = {
                "methods_errors_weight_avg": methods_errors_weight_avg
            }

            only_predict_result_one = {
                "methods_errors_one": methods_errors_one
                }
            

        self.visualization_effiency(meta_result, bins_base, name="full_base")
        self.visualization_effiency(meta_result, bins_hight, name="full_hight")

        self.visualization_effiency(
            only_predict_result_avg, bins_base,(6,6), name="avg_base"
            )
        self.visualization_effiency(
            only_predict_result_avg, bins_hight, (8,6), name="avg_hight"
            )

        self.visualization_effiency(
            only_predict_result_one, bins_base, (6,6), name="one_base"
            )
        self.visualization_effiency(
            only_predict_result_one, bins_hight, (8,6), name="one_hight"
            )

        return 

    def visualization_effiency(self, meta_result, bins, figsize = (12, 8), name="full"):
        """
        Visualizes the efficiency of different methods using a histogram plot.
        
        This function takes in a dictionary of results from various methods and a set of bins to use for the histogram. It then creates a dataframe from the results, calculates the histogram counts for each bin, and plots the histogram using Matplotlib.
        
        The function also includes helper functions to create the dataframe, calculate the histogram, plot the histogram, and automatically generate the category labels for the x-axis.
        
        Args:
            meta_result (dict): A dictionary containing the results from various methods.
            bins (list): A list of bin edges to use for the histogram.
        
        Returns:
            None
        """
                
        def create_dataframe(data, column_name='error'):
            return pd.DataFrame(data, columns=[column_name])

        def calculate_histogram(df, bins):
            return np.histogram(df.error, bins=bins)

        def plot_histogram(data_dict, categories, figsize=figsize):
            fig, ax = plt.subplots(figsize=figsize, layout='constrained')
            bar_width = 0.8 / len(data_dict)
            x = np.arange(len(categories))

            for i, (label, data) in enumerate(data_dict.items()):
                offset = (i - len(data_dict)/2 + 0.5) * bar_width
                bars = ax.bar(x + offset, data, bar_width, label=label)
                autolabel(ax, bars)

            ax.set_xticks(x)
            ax.set_xticklabels(categories)
            ax.set_yticks(np.arange(0, 1000, 100))
            ax.set_xlabel('Погрешность выбранных методов, %')
            ax.set_ylabel('Количество элементов входящих в промежуток')
            ax.grid(True, linestyle="-", color="0.75")
            fig.legend(loc='outside upper right')

            ## TODO: Reconstruct structures of the file path accounting 
            plt.savefig("graf\\compose\\classification_results_"+name+"_" 
                    + datetime.datetime.now().strftime("%d_%m_%Y_%H_%M")
                    + ".png", dpi=500, bbox_inches="tight")
            plt.show()

        def autolabel(ax, rects):
            for rect in rects:
                height = rect.get_height()
                ax.annotate(f'{height}', xy=(rect.get_x() + rect.get_width() / 2, height),
                            xytext=(0, 3), textcoords="offset points",
                            ha='center', va='bottom')

        def process_and_plot_results(result_dict, bins, categories):
            data_dict = {}
            for label, result in result_dict.items():
                df = create_dataframe(result)
                counts, _ = calculate_histogram(df, bins)
                data_dict[label] = counts

            plot_histogram(data_dict, categories)

        def autocategoies(bins):
            categories = []
            for i in range(len(bins) - 1):
                if bins[i+1] != np.inf:
                    categories.append(f"({bins[i]},{bins[i+1]}]")
                else:
                    categories.append(f"({bins[-2]}, Ꝏ)")
            return categories

        process_and_plot_results(meta_result,bins,autocategoies(bins))

    def weigth_avg_method(
                    self, 
                    ErrorsPerMethod: np.ndarray, 
                    marker_test: np.ndarray, 
                    Y_pred_proba: np.ndarray):
        """
        Calculates the absolute mean error for each experiment using a weighted average method.
        
        This method takes in the errors per method, the marker test data, and the predicted probabilities for each experiment. It then calculates the absolute mean error for each experiment using a weighted average of the errors, where the weights are the predicted probabilities for each method. The resulting errors for each experiment are returned as an array.
        
        Args:
            ErrorsPerMethod (np.ndarray): A 2D numpy array containing the errors for each method and each experiment.
            marker_test (np.ndarray): A 1D numpy array containing the experiment indices for the test data.
            Y_pred_proba (np.ndarray): A 2D numpy array containing the predicted probabilities for each method and each experiment.
        
        Returns:
            np.ndarray: An array of the absolute mean errors for each experiment.
        """    

        methods_errors = []

        for idx in range(len(Y_pred_proba)):
            exp = marker_test[idx][0]
            methods_errors.append(np.abs(
                np.mean(
                    ErrorsPerMethod[exp] * Y_pred_proba[idx])))
        
        return np.array(methods_errors)
    
    def one_method(
            self,
            ErrorsPerMethod: np.ndarray,
            marker_test: np.ndarray,
            Y_pred: np.ndarray):
        """
        Calculates the absolute mean error for each experiment using the one_method approach.
        
        This method takes in the errors per method, the marker test data, and the predicted values for each experiment. It then calculates the absolute mean error for each experiment using the one_method approach and returns an array of the errors for each experiment.
        
        Args:
            ErrorsPerMethod (np.ndarray): A 2D numpy array containing the errors for each method and each experiment.
            marker_test (np.ndarray): A 1D numpy array containing the experiment indices for the test data.
            Y_pred (np.ndarray): A 2D numpy array containing the predicted values for each experiment.
        
        Returns:
            np.ndarray: An array of the absolute mean errors for each experiment.
        """
          
        methods_errors = []

        for idx in range(len(Y_pred)):
            method = Y_pred[idx][0] - 1
            exp = marker_test[idx][0]
            methods_errors.append(np.abs(ErrorsPerMethod[exp][method]))

        return np.array(methods_errors)
            
    def STO_method(
            self,
            ErrorsPerMethod: np.ndarray,
            marker_test: np.ndarray
            ):
        """
        Calculates the absolute mean error for each experiment using the STO (Single Test Observation) method.
        
        This method takes in the errors per method and the marker test data, and calculates the absolute mean error for each experiment. It then returns an array of the errors for each experiment.
        
        Args:
            ErrorsPerMethod (np.ndarray): A 2D numpy array containing the errors for each method and each experiment.
            marker_test (np.ndarray): A 1D numpy array containing the experiment indices for the test data.
        
        Returns:
            np.ndarray: An array of the absolute mean errors for each experiment.
        """
                
        methods_errors = []

        for idx in range(len(marker_test)):
            exp = marker_test[idx][0]
            methods_errors.append(np.abs(np.mean(ErrorsPerMethod[exp])))

        return np.array(methods_errors)

    def methods_for_test(
                        self,
                        ErrorsPerMethod: np.ndarray,
                        marker_test: np.ndarray,
                        MethodsList: list):
        """
        Calculates the errors for each method on the test dataset.
        
        This method takes in the errors per method, the marker test data, and a list of method names. It then calculates the absolute mean error for each method and returns a DataFrame containing the errors for each method.
        
        Args:
            ErrorsPerMethod (np.ndarray): A 2D numpy array containing the errors for each method and each experiment.
            marker_test (np.ndarray): A 1D numpy array containing the experiment indices for the test data.
            MethodsList (list): A list of method names.
        
        Returns:
            np.ndarray: A DataFrame containing the errors for each method.
        """
        
        method_errors_df = pd.DataFrame(columns=MethodsList)

        for method in MethodsList:
            method_errors = []
            for idx in range(len(marker_test)):
                exp = marker_test[idx][0]
                method_errors.append(
                    np.abs(np.mean(ErrorsPerMethod[exp][method]))
                )
            method_errors_df[method] = method_errors
        
        return np.array(method_errors_df)
    
    def main(self):
        """
        Executes the main workflow of the time series classification model.
        
        This method performs the following steps:
        1. Preprocess the data using the `data_preprocess()` method.
        2. Train the time series classification model using the `train_model()` method.
        3. Evaluate the performance of the trained model on the test dataset using the `test_model()` method.
        4. Return the trained model.
        
        Returns:
            TimeSeriesForestClassifier: The trained time series classification model.
        """
        self = self.data_preprocess()
        self = self.train_model()
        accuracy, report = self.test_model()
        return self.model, accuracy, report